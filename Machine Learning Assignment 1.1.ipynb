{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer #More in-depth: Use avg of K nearest neighbours\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in all available data\n",
    "trainingData = pd.read_csv(\"training.csv\")\n",
    "testData = pd.read_csv(\"testing.csv\")\n",
    "extraData = pd.read_csv('additional_training.csv')\n",
    "annotationData = pd.read_csv(\"annotation_confidence.csv\")\n",
    "annotationConfidence = annotationData.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the CNN and GIST features\n",
    "CNNFeatures = trainingData.iloc[:,1:4097]\n",
    "GISTFeatures = trainingData.iloc[:,4097:4609]\n",
    "expectedOutput1 = trainingData.iloc[:,4609]\n",
    "test_CNN = testData.iloc[:,1:4097]\n",
    "test_GIST = testData.iloc[:,4097:4609]\n",
    "\n",
    "additionalCNN = extraData.iloc[:,1:4097]\n",
    "additionalGIST = extraData.iloc[:,4097:4609]\n",
    "\n",
    "#Join the two CNN training data sets\n",
    "np1 = np.asarray(CNNFeatures)\n",
    "np2 = np.asarray(additionalCNN)\n",
    "CNN_raw = np.concatenate((np1, np2), axis = 0)\n",
    "#Join the two GIST training data sets\n",
    "np1 = np.asarray(GISTFeatures)\n",
    "np2 = np.asarray(additionalGIST)\n",
    "GIST_raw = np.concatenate((np1, np2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add extra training data results\n",
    "additionalExpectedOutputs = extraData.iloc[:,4609]\n",
    "np3 = np.asarray(expectedOutput1)\n",
    "np4 = np.asarray(additionalExpectedOutputs)\n",
    "expectedOutput = np.concatenate((np3, np4), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Nearest Neighbours Impute\n",
    "\n",
    "#Impute extra training missing values CNN\n",
    "imp = KNNImputer(n_neighbors=2)\n",
    "train_CNN = imp.fit_transform(CNN_raw)\n",
    "#Impute extra training missing values GIST\n",
    "train_GIST = imp.fit_transform(GIST_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data CNN\n",
    "scaler = StandardScaler()\n",
    "#scaler.fit(train_CNN)\n",
    "scaledTrain_CNN = scaler.fit_transform(train_CNN)#train_CNN\n",
    "scaledTest_CNN = scaler.fit_transform(test_CNN)\n",
    "#Standardize the data GIST\n",
    "scaler.fit(train_GIST)\n",
    "scaledTrain_GIST = scaler.transform(train_GIST)\n",
    "scaledTest_GIST = scaler.transform(testGISTFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaptation Problem - Attempt to address class imbalace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ratio:  0.8678021086780211 |1 number:  2140\n",
      "0 ratio:  0.1321978913219789 |0 number:  326\n",
      "\n",
      "Therefore the following amount of +ve cases with low confidence must be flipped in training data to match testing proportions: \n",
      "1191\n"
     ]
    }
   ],
   "source": [
    "#Check training proportions, highlights class imbalance - part of domain adaptation problem\n",
    "memorable = 0\n",
    "nonMemorable = 0\n",
    "for x in range(len(expectedOutput)):#expectedOutput\n",
    "    if expectedOutput[x] == 1: #expectedOutput\n",
    "        memorable += 1\n",
    "    else:\n",
    "         nonMemorable = nonMemorable + 1        \n",
    "            \n",
    "#Display amount + ratio of each class in training data\n",
    "print(\"1 ratio: \", memorable/len(expectedOutput), \"|1 number: \", memorable)     #Testing 1 proportion = 0.3848\n",
    "print(\"0 ratio: \", nonMemorable/len(expectedOutput),\"|0 number: \", nonMemorable)#Testing 0 proportion = 0.6152\n",
    "\n",
    "print(\"\")\n",
    "print(\"Therefore the following amount of +ve cases with low confidence must be flipped in training data to match testing proportions: \")\n",
    "ratioToFlip = ((memorable/len(expectedOutput))-0.3848)#Difference between memoroable train + test proportions \n",
    "numberToFlip = round(ratioToFlip * 2466)#2466 = total no of training entries\n",
    "print(numberToFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "#Flip training 1's with low confidence to bridge train/test class imbalance\n",
    "flipped = 0\n",
    "index = 0\n",
    "while flipped < numberToFlip and index < len(expectedOutput):\n",
    "    if expectedOutput[index] == 1 and annotationConfidence[index] != 1:\n",
    "        expectedOutput[index] = 0\n",
    "        flipped += 1\n",
    "    index += 1\n",
    "\n",
    "#Only exist 998 +ve entries with confidence < 1\n",
    "print(flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length before resampling: 2466\n",
      "Adjusted dataset length: 2079\n",
      "1 ratio:  0.5493025493025493 | 1 number:  800\n",
      "0 ratio:  0.6368446368446369 | 0 number:  1279\n"
     ]
    }
   ],
   "source": [
    "#Random Undersampling to exactly match training/testing proportions\n",
    "trainingDataToResample = scaledTrain_CNN #Adjust to control which pre-processing methods are used\n",
    "sampler = RandomUnderSampler(sampling_strategy = {1: 800, 0:1279}) \n",
    "\n",
    "print('Dataset length before resampling: %s' % len(expectedOutput))\n",
    "CNN, expectedOutput_res = sampler.fit_resample(trainingDataToResample, expectedOutput.ravel()) #was train_CNN for best score\n",
    "print('Adjusted dataset length: %s' % len(expectedOutput_res))\n",
    "\n",
    "print(\"1 ratio: \",Counter(expectedOutput.ravel())[1]/len(expectedOutput_res), \"| 1 number: \", Counter(expectedOutput_res.ravel())[1])\n",
    "print(\"0 ratio: \",Counter(expectedOutput.ravel())[0]/len(expectedOutput_res), \"| 0 number: \", Counter(expectedOutput_res.ravel())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.4831730769230769, 0.5721153846153846, 0.1971153846153846, 0.13253012048192772]\n",
      "0.3769867933271548\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "score = []\n",
    "count = 0\n",
    "\n",
    "for train_index, test_index in kf.split(CNN): \n",
    "    X_train, X_test = CNN[train_index], CNN[test_index]\n",
    "    y_train, y_test = expectedOutput_res[train_index], expectedOutput_res[test_index]\n",
    "    \n",
    "    classifier = MLPClassifier(max_iter = 500) #WithoutFlips - Impute+Scaled, 84.67% local, 58.83% Kaggle, domain adaptation problem\n",
    "                                 #WithFlips - Impute+Scaled, 51.17% local,\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    \n",
    "    score.append(classifier.score(X_test, y_test))\n",
    "    \n",
    "    count+=1\n",
    "\n",
    "print(score)\n",
    "print(statistics.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Retrain using full dataset ready for Kaggle\n",
    "\n",
    "#Initialize and train classifier\n",
    "classifier = MLPClassifier(max_iter = 500)\n",
    "classifier.fit(CNN, expectedOutput_res)\n",
    "\n",
    "#Predict\n",
    "predictions = classifier.predict(scaledTest_CNN)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       prediction\n",
      "id               \n",
      "1               1\n",
      "2               0\n",
      "3               1\n",
      "4               0\n",
      "5               0\n",
      "...           ...\n",
      "11870           1\n",
      "11871           1\n",
      "11872           0\n",
      "11873           0\n",
      "11874           0\n",
      "\n",
      "[11874 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "evalPredictions = predictions #adjust which predictions to evaluate\n",
    "#Convert to dataframe\n",
    "predictionsNumpy = np.asarray(evalPredictions)\n",
    "predictionsArray = predictionsNumpy.tolist()\n",
    "predictionsDf = pd.DataFrame(data = predictionsArray, index=list(range(1, len(evalPredictions)+1)))\n",
    "\n",
    "#Index from 1, label colums\n",
    "toExportDf = predictionsDf\n",
    "toExportDf.columns = ['prediction']\n",
    "toExportDf.index = np.arange(1, len(toExportDf)+1)\n",
    "toExportDf.index.name = 'id'\n",
    "\n",
    "#Export to csv\n",
    "print(toExportDf)\n",
    "toExportDf.to_csv(\"Attempt_35.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing methods not currently in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarizer CNN   -------- NOT IN USE-------------\n",
    "binarizer = preprocessing.Binarizer(threshold = 0.293)\n",
    "binarised_scaled_trainCNN = binarizer.transform(scaledTrain_CNN)\n",
    "binarised_scaled_testCNN = binarizer.transform(scaledTest_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KernelPCA   -------- NOT IN USE------------\n",
    "kpca = KernelPCA(n_components = 2440, kernel='precomputed')\n",
    "KPCAtest_CNN = kpca.fit_transform(scaledTest_CNN)\n",
    "KPCAtrain_CNN = kpca.fit_transform(scaledTrain_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization -------- NOT IN USE------------\n",
    "norm = Normalizer()\n",
    "normTrain_CNN = norm.fit_transform(train_CNN)\n",
    "normTest_CNN = norm.fit_transform(test_CNN)\n",
    "normTrain_GIST = norm.fit_transform(train_GIST)\n",
    "normTest_GIST = norm.fit_transform(test_GIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA  -------- NOT IN USE------------\n",
    "pca = PCA(n_components = 100)#n_components = 100\n",
    "PCAtest_CNN = pca.fit_transform(test_CNN)\n",
    "PCAtrain_CNN = pca.fit_transform(train_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Impute  -------- NOT IN USE------------\n",
    "\n",
    "#Impute extra training missing values CNN\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(CNN_raw)\n",
    "train_CNN = imp.transform(CNN_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
